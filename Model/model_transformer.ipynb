{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class TorchTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=32, num_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim, num_heads=num_heads,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ln1(self.input_proj(x))\n",
    "        x = x.unsqueeze(1)\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.ln2(x + attn_out)\n",
    "        x = self.ln3(x + self.ffn(x))\n",
    "        x = x.squeeze(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class TorchTransformerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, in_dim=None, hidden_dim=32, num_heads=4, dropout=0.3, \n",
    "                 epochs=200, lr=5e-4, batch_size=32, patience=30,\n",
    "                 device=None, verbose=True, random_state=42):\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        set_seed(self.random_state)\n",
    "        \n",
    "        if self.in_dim is None:\n",
    "            self.in_dim = X.shape[1]\n",
    "        \n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X, y, test_size=0.15, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(X_tr, dtype=torch.float32),\n",
    "            torch.tensor(y_tr, dtype=torch.long)\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        X_val_t = torch.tensor(X_val, dtype=torch.float32).to(self.device)\n",
    "        y_val_t = torch.tensor(y_val, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        self.model = TorchTransformer(\n",
    "            self.in_dim, \n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=self.dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=10, verbose=False\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\" Training Transformer...\")\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred = self.model(batch_X)\n",
    "                loss = criterion(pred, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * batch_X.size(0)\n",
    "            \n",
    "            train_loss /= len(train_dataset)\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = self.model(X_val_t)\n",
    "                val_loss = criterion(val_pred, y_val_t).item()\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"  Early stopping at epoch {epoch+1}, best val_loss: {best_val_loss:.4f}\")\n",
    "                    break\n",
    "            \n",
    "            if self.verbose and (epoch + 1) % 50 == 0:\n",
    "                print(f\"  Epoch {epoch+1}, Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "        \n",
    "        if best_state is not None:\n",
    "            self.model.load_state_dict(best_state)\n",
    "            self.model.to(self.device)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\" Transformer done! Best val_loss: {best_val_loss:.4f}\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        dataset = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        all_proba = []\n",
    "        with torch.no_grad():\n",
    "            for (batch_X,) in loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                logits = self.model(batch_X)\n",
    "                proba = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                all_proba.append(proba)\n",
    "        \n",
    "        return np.vstack(all_proba)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir('G:\\lhy\\work')\n",
    "    \n",
    "    X_new = pd.read_csv(r'G:\\\\lhy\\work\\\\train_feature\\\\feature\\\\ESMC300+PAAC+APAAC.csv', header=None)\n",
    "    y_new = pd.read_csv(r'data/label_train.csv', header=None)\n",
    "    X_new1 = pd.read_csv(r\"G:\\lhy\\work\\test_feature\\ESMC300+PAAC+APAAC.csv\", header=None)\n",
    "    y_new1 = pd.read_csv(r'data/label_test.csv', header=None)\n",
    "    \n",
    "    X_train = np.array(X_new)\n",
    "    y_train = np.array(y_new).ravel()\n",
    "    X_test = np.array(X_new1)\n",
    "    y_test = np.array(y_new1).ravel()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\" {X_train.shape}\")\n",
    "    print(f\" {X_test.shape}\")\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    transformer_classifier = TorchTransformerClassifier(\n",
    "        in_dim=input_dim,\n",
    "        hidden_dim=32,\n",
    "        num_heads=4,\n",
    "        dropout=0.3,\n",
    "        epochs=200,\n",
    "        lr=5e-4,\n",
    "        batch_size=32,\n",
    "        patience=30,\n",
    "        verbose=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    transformer_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = transformer_classifier.predict(X_test)\n",
    "    y_proba = transformer_classifier.predict_proba(X_test)\n",
    "    \n",
    "    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    mcc = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) + 1e-10)\n",
    "    auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"MCC:         {mcc:.4f}\")\n",
    "    print(f\"AUC:         {auc:.4f}\")\n",
    "    print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
